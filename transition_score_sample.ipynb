{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPKLJ1lJhTLjHSszFKeoXBu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AoShuang92/PhD_tutorial/blob/main/transition_score_sample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_gzNHYVMrQO2"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer, AutoModelForCausalLM\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "inputs = tokenizer([\"Today is\"], return_tensors=\"pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YK2QV5vu9sF",
        "outputId": "588e3c4e-2aef-4ecf-857c-b584331795d2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(**inputs, max_new_tokens=5, return_dict_in_generate=True, output_scores=True)\n",
        "outputs.keys(), outputs.scores[0].shape, outputs.scores[0][:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FH5tG-bxDKW",
        "outputId": "6e7687d4-fd4b-4008-ab32-9dd7f4c4f060"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(odict_keys(['sequences', 'scores', 'past_key_values']),\n",
              " torch.Size([1, 50257]),\n",
              " tensor([[-122.8355, -122.5403, -127.6362,  ..., -133.4906, -131.9769,\n",
              "          -125.4615]]))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs.sequences, tokenizer.decode(outputs.sequences[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xn2UoqR4ZLo",
        "outputId": "63427bba-5008-4ee3-e57c-32f888357a9f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[8888,  318,  262, 1110,  618,  356,  460]]),\n",
              " 'Today is the day when we can')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transition_scores = model.compute_transition_scores(outputs.sequences, outputs.scores, normalize_logits=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G421Dv0rvHw9",
        "outputId": "d0c097dc-4e5f-4bb6-fae2-2bb5d3e162bc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "scores0 torch.Size([50257, 5])\n",
            "scores1 torch.Size([1, 50257, 5])\n",
            "scores2 tensor([[[ -9.5134, -13.5121,  -4.5552,  -9.2214, -10.0484],\n",
            "         [ -9.2181, -11.9378,  -6.2420,  -9.6079, -10.7442],\n",
            "         [-14.3140, -15.1278, -14.0803, -14.2024, -15.1110],\n",
            "         ...,\n",
            "         [-20.1684, -18.0281, -17.5477, -14.8852, -21.2452],\n",
            "         [-18.6548, -19.5730, -17.9392, -16.2873, -14.1327],\n",
            "         [-12.1394, -12.0541,  -9.2910, -10.5160, -11.6027]]]) torch.Size([1, 50257, 5])\n",
            "scores3 torch.Size([50257, 5])\n",
            "cut_idx tensor(2) tensor(5)\n",
            "indices tensor([[ 262, 1110,  618,  356,  460]]) 1 tensor([[0, 0, 0, 0, 0]])\n",
            "transition_scores tensor([[-1.4135, -2.6088, -2.0095, -1.8592, -2.5082]])\n",
            "beam_indices_mask tensor([[False, False, False, False, False]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transition_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgCi3hAlviv2",
        "outputId": "faab2b6c-76b6-4b8f-de0d-3858362ff3cc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.4135, -2.6088, -2.0095, -1.8592, -2.5082]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n",
        "generated_tokens = outputs.sequences[:, input_length:]\n",
        "for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
        "    print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy():.3f} | {np.exp(score.numpy()):.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dliM6my_vR1Y",
        "outputId": "a679a9b8-a570-4dde-9e09-34ac57285015"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   262 |  the     | -1.414 | 24.33%\n",
            "|  1110 |  day     | -2.609 | 7.36%\n",
            "|   618 |  when    | -2.010 | 13.41%\n",
            "|   356 |  we      | -1.859 | 15.58%\n",
            "|   460 |  can     | -2.508 | 8.14%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#source code for model.compute_transition_scores\n",
        "def compute_transition_scores(\n",
        "        self,\n",
        "        sequences: torch.Tensor,\n",
        "        scores: Tuple[torch.Tensor],\n",
        "        beam_indices: Optional[torch.Tensor] = None,\n",
        "        normalize_logits: bool = False,\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Computes the transition scores of sequences given the generation scores (and beam indices, if beam search was\n",
        "        used). This is a convenient method to quicky obtain the scores of the selected tokens at generation time.\n",
        "\n",
        "        Parameters:\n",
        "            sequences (`torch.LongTensor`):\n",
        "                The generated sequences. The second dimension (sequence_length) is either equal to `max_length` or\n",
        "                shorter if all batches finished early due to the `eos_token_id`.\n",
        "            scores (`tuple(torch.FloatTensor)`):\n",
        "                Transition scores for each vocabulary token at each generation step. Beam transition scores consisting\n",
        "                of log probabilities of tokens conditioned on log softmax of previously generated tokens in this beam.\n",
        "                Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for each generated token),\n",
        "                with each tensor of shape `(batch_size*num_beams, config.vocab_size)`.\n",
        "            beam_indices (`torch.LongTensor`, *optional*):\n",
        "                Beam indices of generated token id at each generation step. `torch.LongTensor` of shape\n",
        "                `(batch_size*num_return_sequences, sequence_length)`. Only required if a `num_beams>1` at\n",
        "                generate-time.\n",
        "            normalize_logits (`bool`, *optional*, defaults to `False`):\n",
        "                Whether to normalize the logits (which, for legacy reasons, may be unnormalized).\n",
        "\n",
        "        Return:\n",
        "            `torch.Tensor`: A `torch.Tensor` of shape `(batch_size*num_return_sequences, sequence_length)` containing\n",
        "                the transition scores (logits)\n",
        "\n",
        "        Examples:\n",
        "\n",
        "        ```python\n",
        "        >>> from transformers import GPT2Tokenizer, AutoModelForCausalLM\n",
        "        >>> import numpy as np\n",
        "\n",
        "        >>> tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "        >>> model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
        "        >>> tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "        >>> inputs = tokenizer([\"Today is\"], return_tensors=\"pt\")\n",
        "\n",
        "        >>> # Example 1: Print the scores for each token generated with Greedy Search\n",
        "        >>> outputs = model.generate(**inputs, max_new_tokens=5, return_dict_in_generate=True, output_scores=True)\n",
        "        >>> transition_scores = model.compute_transition_scores(\n",
        "        ...     outputs.sequences, outputs.scores, normalize_logits=True\n",
        "        ... )\n",
        "        >>> # input_length is the length of the input prompt for decoder-only models, like the GPT family, and 1 for\n",
        "        >>> # encoder-decoder models, like BART or T5.\n",
        "        >>> input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n",
        "        >>> generated_tokens = outputs.sequences[:, input_length:]\n",
        "        >>> for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
        "        ...     # | token | token string | log probability | probability\n",
        "        ...     print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy():.3f} | {np.exp(score.numpy()):.2%}\")\n",
        "        |   262 |  the     | -1.414 | 24.33%\n",
        "        |  1110 |  day     | -2.609 | 7.36%\n",
        "        |   618 |  when    | -2.010 | 13.40%\n",
        "        |   356 |  we      | -1.859 | 15.58%\n",
        "        |   460 |  can     | -2.508 | 8.14%\n",
        "\n",
        "        >>> # Example 2: Reconstruct the sequence scores from Beam Search\n",
        "        >>> outputs = model.generate(\n",
        "        ...     **inputs,\n",
        "        ...     max_new_tokens=5,\n",
        "        ...     num_beams=4,\n",
        "        ...     num_return_sequences=4,\n",
        "        ...     return_dict_in_generate=True,\n",
        "        ...     output_scores=True,\n",
        "        ... )\n",
        "        >>> transition_scores = model.compute_transition_scores(\n",
        "        ...     outputs.sequences, outputs.scores, outputs.beam_indices, normalize_logits=False\n",
        "        ... )\n",
        "        >>> # If you sum the generated tokens' scores and apply the length penalty, you'll get the sequence scores.\n",
        "        >>> # Tip 1: recomputing the scores is only guaranteed to match with `normalize_logits=False`. Depending on the\n",
        "        >>> # use case, you might want to recompute it with `normalize_logits=True`.\n",
        "        >>> # Tip 2: the output length does NOT include the input length\n",
        "        >>> output_length = np.sum(transition_scores.numpy() < 0, axis=1)\n",
        "        >>> length_penalty = model.generation_config.length_penalty\n",
        "        >>> reconstructed_scores = transition_scores.sum(axis=1) / (output_length**length_penalty)\n",
        "        >>> print(np.allclose(outputs.sequences_scores, reconstructed_scores))\n",
        "        True\n",
        "        ```\"\"\"\n",
        "        # 1. In absence of `beam_indices`, we can assume that we come from e.g. greedy search, which is equivalent\n",
        "        # to a beam search approach were the first (and only) beam is always selected\n",
        "        if beam_indices is None:\n",
        "            beam_indices = torch.arange(scores[0].shape[0]).view(-1, 1).to(sequences.device)\n",
        "            beam_indices = beam_indices.expand(-1, len(scores))\n",
        "\n",
        "        # 2. reshape scores as [batch_size*vocab_size, # generation steps] with # generation steps being\n",
        "        # seq_len - input_length\n",
        "        scores = torch.stack(scores).reshape(len(scores), -1).transpose(0, 1)\n",
        "        print('scores0', scores.shape)\n",
        "        # 3. Optionally normalize the logits (across the vocab dimension)\n",
        "        if normalize_logits:\n",
        "            scores = scores.reshape(-1, self.config.vocab_size, scores.shape[-1])\n",
        "            print(\"scores1\", scores.shape)\n",
        "            scores = torch.nn.functional.log_softmax(scores, dim=1)\n",
        "            print(\"scores2\", scores, scores.shape)\n",
        "            scores = scores.reshape(-1, scores.shape[-1])\n",
        "            print(\"scores3\", scores.shape)\n",
        "\n",
        "        # 4. cut beam_indices to longest beam length\n",
        "        beam_indices_mask = beam_indices < 0\n",
        "        max_beam_length = (1 - beam_indices_mask.long()).sum(-1).max()\n",
        "        beam_indices = beam_indices.clone()[:, :max_beam_length]\n",
        "        beam_indices_mask = beam_indices_mask[:, :max_beam_length]\n",
        "\n",
        "        # 5. Set indices of beams that finished early to 0; such indices will be masked correctly afterwards\n",
        "        beam_indices[beam_indices_mask] = 0\n",
        "\n",
        "        # 6. multiply beam_indices with vocab size to gather correctly from scores\n",
        "        beam_sequence_indices = beam_indices * self.config.vocab_size\n",
        "\n",
        "        # 7. Define which indices contributed to scores\n",
        "        cut_idx = sequences.shape[-1] - max_beam_length\n",
        "        print(\"cut_idx\", cut_idx, max_beam_length)\n",
        "        indices = sequences[:, cut_idx:] + beam_sequence_indices\n",
        "        print(\"indices\", indices, len(indices), beam_sequence_indices)\n",
        "        # 8. Compute scores\n",
        "        transition_scores = scores.gather(0, indices)\n",
        "        print('transition_scores', transition_scores)\n",
        "\n",
        "        # 9. Mask out transition_scores of beams that stopped early\n",
        "        transition_scores[beam_indices_mask] = 0\n",
        "        print('beam_indices_mask', beam_indices_mask)\n",
        "        return transition_scores"
      ],
      "metadata": {
        "id": "yxwDJQ6hvfUo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}